{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "===      第 5.1.3 节: 数据预处理策略 (数据采样)      ===\n",
      "====================================================\n",
      "\n",
      "[INFO] 已成功从 'data\\processed\\final_processed.csv' 加载数据集。\n",
      "数据集形状: (4312063, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 机器学习和数据处理库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# imbalanced-learn 库，专门用于处理类别不均衡问题\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 可视化库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"====================================================\")\n",
    "print(\"===      第 5.1.3 节: 数据预处理策略 (数据采样)      ===\")\n",
    "print(\"====================================================\")\n",
    "\n",
    "# --- 步骤 1: 加载已处理好的最终数据集 ---\n",
    "processed_data_path = os.path.join('data', 'processed', 'final_processed.csv')\n",
    "\n",
    "try:\n",
    "    final_df = pd.read_csv(processed_data_path)\n",
    "    print(f\"\\n[INFO] 已成功从 '{processed_data_path}' 加载数据集。\")\n",
    "    print(f\"数据集形状: {final_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[ERROR] 文件未找到: '{processed_data_path}'。请确保您已运行并保存了之前的预处理代码。\")\n",
    "    # 如果文件不存在，退出后续操作\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 正在对数据进行编码...\n",
      "\n",
      "[INFO] 正在统一目标标签的名称...\n",
      "标签名称统一完成。统一后的标签分布情况:\n",
      "Label\n",
      "Legitimate      3072442\n",
      "TCPSYN-flood     610074\n",
      "UDP-flood        335018\n",
      "ICMP-flood       124847\n",
      "DNS-flood        102745\n",
      "HTTP-flood        66937\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 步骤 2: 数据编码 (将所有数据转换为数值型) ---\n",
    "print(\"\\n[INFO] 正在对数据进行编码...\")\n",
    "\n",
    "# 2.0 统一目标标签名称\n",
    "#     确保'Legitimate'和'Legitimate_traffic'等统一为一个名称\n",
    "print(\"\\n[INFO] 正在统一目标标签的名称...\")\n",
    "label_mapping_dict = {\n",
    "    'Legitimate': 'Legitimate',\n",
    "    'TCPSYN-flood': 'TCPSYN-flood',\n",
    "    'UDP-flood': 'UDP-flood',\n",
    "    # 需要统一的标签\n",
    "    'Legitimate_traffic': 'Legitimate',\n",
    "    'TCP_syn_flood_attack': 'TCPSYN-flood',\n",
    "    'UDP_flood_attack': 'UDP-flood',\n",
    "    'ICMP_flood_attack': 'ICMP-flood',\n",
    "    'DNS_flood_attack': 'DNS-flood',\n",
    "    'HTTP_flood_attack': 'HTTP-flood'\n",
    "}\n",
    "\n",
    "# 使用 .map() 函数高效地应用这个映射\n",
    "final_df['Label'] = final_df['Label'].map(label_mapping_dict)\n",
    "\n",
    "print(\"标签名称统一完成。统一后的标签分布情况:\")\n",
    "print(final_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征集 X 编码完成。新的特征形状: (4312063, 19)\n",
      "新的特征列: ['Timestamp', 'TTL', 'Length', 'SYN', 'ACK', 'RST', 'PSH', 'FIN', 'Protocol_DNS', 'Protocol_Generic Routing Encapsulation', 'Protocol_HTTP', 'Protocol_ICMP', 'Protocol_ICMP,ICMP', 'Protocol_ICMP,TCP', 'Protocol_ICMP,UDP', 'Protocol_IGMP', 'Protocol_IPv6', 'Protocol_TCP', 'Protocol_UDP']...\n"
     ]
    }
   ],
   "source": [
    "# 2.1 编码特征 (X)\n",
    "# 'Protocol' 是类别型特征，我们使用 One-Hot 编码 (pd.get_dummies)\n",
    "# 这会为每个协议类型创建一个新的二元 (0/1) 列，避免了模型错误地理解协议之间的顺序关系\n",
    "X = pd.get_dummies(final_df.drop('Label', axis=1), columns=['Protocol'], dtype=int)\n",
    "print(f\"特征集 X 编码完成。新的特征形状: {X.shape}\")\n",
    "print(f\"新的特征列: {X.columns.tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "目标标签 y 编码完成。\n",
      "标签映射关系 (编码 -> 原始标签):\n",
      "{0: 'DNS-flood', 1: 'HTTP-flood', 2: 'ICMP-flood', 3: 'Legitimate', 4: 'TCPSYN-flood', 5: 'UDP-flood'}\n"
     ]
    }
   ],
   "source": [
    "# 2.2 编码目标标签 (y)\n",
    "# 'Label' 是我们的目标，我们使用 LabelEncoder 将其转换为整数 (0, 1, 2...)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(final_df['Label'])\n",
    "# 保存编码器和类别映射关系，以便后续解码\n",
    "label_mapping = {index: label for index, label in enumerate(le.classes_)}\n",
    "print(\"\\n目标标签 y 编码完成。\")\n",
    "print(\"标签映射关系 (编码 -> 原始标签):\")\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 数据集已划分为训练集和测试集。\n",
      "训练集形状: X_train=(1293618, 19), y_train=(1293618,)\n",
      "测试集形状: X_test=(3018445, 19), y_test=(3018445,)\n",
      "训练集已保存: 'data\\splitted\\train_original.csv'\n",
      "训练集形状: (1293618, 20)\n",
      "\n",
      "测试集已保存: 'data\\splitted\\test.csv'\n",
      "测试集形状: (3018445, 20)\n",
      "\n",
      "[验证] 训练集类别分布:\n",
      "Counter({np.int64(3): 921732, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "\n",
      "[验证] 测试集类别分布:\n",
      "Counter({np.int64(3): 2150710, np.int64(4): 427052, np.int64(5): 234513, np.int64(2): 87393, np.int64(0): 71921, np.int64(1): 46856})\n",
      "\n",
      "[INFO] 数据已保存到目录: data\\splitted\n"
     ]
    }
   ],
   "source": [
    "# --- 步骤 3: 划分训练集和测试集 ---\n",
    "# 严格按照论文的比例: 30% 训练, 70% 测试\n",
    "# stratify=y: 确保划分后的训练集和测试集中的类别分布与原始数据集相似，这在处理不均衡数据时非常重要\n",
    "# random_state=42: 确保每次运行代码时，划分结果都是一样的，便于复现\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.7, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "print(\"\\n[INFO] 数据集已划分为训练集和测试集。\")\n",
    "print(f\"训练集形状: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"测试集形状: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# 使用绝对路径\n",
    "output_dir = os.path.join('data', 'splitted')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === 保存训练集 ===\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_train_df['Label_code'] = y_train\n",
    "\n",
    "train_path = os.path.join(output_dir, 'train_original.csv')\n",
    "X_train_df.to_csv(train_path, index=False)\n",
    "\n",
    "print(f\"训练集已保存: '{train_path}'\")\n",
    "print(f\"训练集形状: {X_train_df.shape}\")\n",
    "\n",
    "# === 保存测试集 ===\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "X_test_df['Label_code'] = y_test\n",
    "\n",
    "test_path = os.path.join(output_dir, 'test.csv')\n",
    "X_test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"\\n测试集已保存: '{test_path}'\")\n",
    "print(f\"测试集形状: {X_test_df.shape}\")\n",
    "\n",
    "# === 验证保存的数据 ===\n",
    "print(\"\\n[验证] 训练集类别分布:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\n[验证] 测试集类别分布:\")\n",
    "print(Counter(y_test))\n",
    "\n",
    "# # === 可选：同时保存为npy格式 ===\n",
    "# np.save(os.path.join(output_dir, 'X_train.npy'), X_train)\n",
    "# np.save(os.path.join(output_dir, 'y_train.npy'), y_train)\n",
    "# np.save(os.path.join(output_dir, 'X_test.npy'), X_test)\n",
    "# np.save(os.path.join(output_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"\\n[INFO] 数据已保存到目录: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 正在对训练集应用数据采样技术...\n",
      "\n",
      "原始训练集的类别分布:\n",
      "  - DNS-flood: 30824 个样本\n",
      "  - HTTP-flood: 20081 个样本\n",
      "  - ICMP-flood: 37454 个样本\n",
      "  - Legitimate: 921732 个样本\n",
      "  - TCPSYN-flood: 183022 个样本\n",
      "  - UDP-flood: 100505 个样本\n"
     ]
    }
   ],
   "source": [
    "# --- 步骤 4: 应用数据采样技术 (仅对训练集) ---\n",
    "print(\"\\n[INFO] 正在对训练集应用数据采样技术...\")\n",
    "\n",
    "# 打印原始训练集的类别分布\n",
    "print(\"\\n原始训练集的类别分布:\")\n",
    "original_counts = Counter(y_train)\n",
    "for label_code, count in sorted(original_counts.items()):\n",
    "    print(f\"  - {label_mapping[label_code]}: {count} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始训练集的类别分布:\n",
      "Counter({np.int64(3): 921732, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "\n",
      "应用 Tomek Links 后的类别分布:\n",
      "Counter({np.int64(3): 921024, np.int64(4): 183022, np.int64(5): 100271, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "移除的样本数: 942\n",
      "\n",
      "应用 Random UnderSampling 后的类别分布:\n",
      "Counter({np.int64(0): 20081, np.int64(1): 20081, np.int64(2): 20081, np.int64(3): 20081, np.int64(4): 20081, np.int64(5): 20081})\n",
      "最终训练集样本数: 120486\n",
      "\n",
      "数据已保存为CSV文件: 'data\\resampled\\train_TLink_RUS.csv'\n",
      "保存的数据形状: (120486, 20)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# 查看原始训练集分布\n",
    "print(\"原始训练集的类别分布:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# 步骤1: 应用 Tomek Links 移除所有多数类的边界样本\n",
    "tomek = TomekLinks(sampling_strategy='not minority', n_jobs=-1)\n",
    "X_train_tomek, y_train_tomek = tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\n应用 Tomek Links 后的类别分布:\")\n",
    "print(Counter(y_train_tomek))\n",
    "print(f\"移除的样本数: {len(y_train) - len(y_train_tomek)}\")\n",
    "\n",
    "# 步骤2: 应用 Random UnderSampling 使所有类别样本数相等\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_tomek, y_train_tomek)\n",
    "\n",
    "print(\"\\n应用 Random UnderSampling 后的类别分布:\")\n",
    "print(Counter(y_train_resampled))\n",
    "print(f\"最终训练集样本数: {len(y_train_resampled)}\")\n",
    "\n",
    "output_dir = os.path.join('data', 'resampled')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. 将采样后的 X (NumPy数组) 转换回带列名的 DataFrame\n",
    "X_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "\n",
    "# 2. 将采样后的 y (NumPy数组) 直接作为新的一列添加到 X_resampled_df 中\n",
    "#    因为它们的长度和顺序由同一个函数生成，所以保证对齐\n",
    "X_resampled_df['Label_code'] = y_train_resampled\n",
    "\n",
    "# 3. 此时，final_resampled_df 就是 X_resampled_df\n",
    "final_resampled_df = X_resampled_df\n",
    "\n",
    "# 4. 定义保存路径并保存为 CSV 文件\n",
    "output_path_csv = os.path.join(output_dir, 'train_TLink_RUS.csv')\n",
    "final_resampled_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "# 5. 打印保存信息\n",
    "print(f\"\\n数据已保存为CSV文件: '{output_path_csv}'\")\n",
    "print(f\"保存的数据形状: {final_resampled_df.shape}\")\n",
    "\n",
    "# np.save(os.path.join(output_dir, 'X_train_TLink_RUS.npy'), X_train_resampled)\n",
    "# np.save(os.path.join(output_dir, 'y_train_TLink_RUS.npy'), y_train_resampled)\n",
    "\n",
    "# print(f\"\\n数据已保存到: {output_dir}\")\n",
    "# print(f\"- X_train_TLink_RUS.npy: {X_train_resampled.shape}\")\n",
    "# print(f\"- y_train_TLink_RUS.npy: {y_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始训练集的类别分布:\n",
      "Counter({np.int64(3): 921732, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "\n",
      "多数类: 3, 少数类: [np.int64(5), np.int64(1), np.int64(4), np.int64(0), np.int64(2)]\n",
      "多数类样本数: 921732\n",
      "少数类样本数: 371886\n",
      "\n",
      "开始 MiniBatch K-Means 聚类，聚类数: 10\n",
      "聚类完成\n",
      "簇 0: 样本数=127019, 比例=0.1378, 采样数=51247\n",
      "簇 1: 样本数=112421, 比例=0.1220, 采样数=45357\n",
      "簇 2: 样本数=152946, 比例=0.1659, 采样数=61708\n",
      "簇 3: 样本数=120599, 比例=0.1308, 采样数=48657\n",
      "簇 4: 样本数=5752, 比例=0.0062, 采样数=2320\n",
      "簇 5: 样本数=296585, 比例=0.3218, 采样数=119661\n",
      "簇 6: 样本数=13739, 比例=0.0149, 采样数=5543\n",
      "簇 7: 样本数=32356, 比例=0.0351, 采样数=13054\n",
      "簇 8: 样本数=3936, 比例=0.0043, 采样数=1588\n",
      "簇 9: 样本数=56379, 比例=0.0612, 采样数=22746\n",
      "\n",
      "采样后的类别分布:\n",
      "Counter({np.int64(3): 743762, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "最终训练集样本数: 1115648\n",
      "\n",
      "数据已保存为CSV文件: 'data\\resampled\\train_CBMP.csv'\n",
      "保存的数据形状: (1115648, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"原始训练集的类别分布:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# 强制转换为 numpy array\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "# 找出多数类\n",
    "class_counts = Counter(y_train_array)\n",
    "majority_class = max(class_counts, key=class_counts.get)\n",
    "minority_classes = [cls for cls in class_counts.keys() if cls != majority_class]\n",
    "\n",
    "print(f\"\\n多数类: {majority_class}, 少数类: {minority_classes}\")\n",
    "\n",
    "# 分离多数类和少数类\n",
    "majority_idx = y_train_array == majority_class\n",
    "minority_idx = ~majority_idx\n",
    "\n",
    "X_majority = X_train_array[majority_idx]\n",
    "y_majority = y_train_array[majority_idx]\n",
    "X_minority = X_train_array[minority_idx]\n",
    "y_minority = y_train_array[minority_idx]\n",
    "\n",
    "print(f\"多数类样本数: {len(y_majority)}\")\n",
    "print(f\"少数类样本数: {len(y_minority)}\")\n",
    "\n",
    "# CBMP 参数设置\n",
    "n_clusters = 10\n",
    "n_minority = len(y_minority)\n",
    "\n",
    "print(f\"\\n开始 MiniBatch K-Means 聚类，聚类数: {n_clusters}\")\n",
    "\n",
    "# 步骤1: 对多数类进行聚类\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=10000, n_init=3, max_iter=100)\n",
    "cluster_labels = kmeans.fit_predict(X_majority)\n",
    "\n",
    "print(\"聚类完成\")\n",
    "\n",
    "# 步骤2: 为每个簇计算采样数量（C1和C2各一半）\n",
    "sampled_indices_random = []  # C1: 随机选择\n",
    "sampled_indices_centroid = []  # C2: 距离簇心最近\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_mask = cluster_labels == i\n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    cluster_size = len(cluster_indices)\n",
    "    \n",
    "    # 计算该簇的采样数量（完整的 s_i，不分半）\n",
    "    r_i = cluster_size / len(y_majority)\n",
    "    s_i = int(r_i * n_minority)\n",
    "    \n",
    "    \n",
    "    print(f\"簇 {i}: 样本数={cluster_size}, 比例={r_i:.4f}, 采样数={s_i}\")\n",
    "    \n",
    "    if s_i > 0:\n",
    "        # 策略1: 随机选择 s_i 个样本\n",
    "        random_sample = np.random.choice(\n",
    "            cluster_indices, \n",
    "            size=min(s_i, cluster_size),  # ← 修改：使用完整的 s_i\n",
    "            replace=False\n",
    "        )\n",
    "        sampled_indices_random.extend(random_sample)\n",
    "        \n",
    "        # 策略2: 选择距离簇心最近的 s_i 个样本\n",
    "        cluster_center = kmeans.cluster_centers_[i]\n",
    "        X_cluster = X_majority[cluster_indices]\n",
    "        distances = np.linalg.norm(X_cluster - cluster_center, axis=1)\n",
    "        closest_indices = cluster_indices[\n",
    "            np.argsort(distances)[:min(s_i, cluster_size)]  # ← 修改：使用完整的 s_i\n",
    "        ]\n",
    "        sampled_indices_centroid.extend(closest_indices)\n",
    "\n",
    "# 步骤3: 合并采样结果\n",
    "X_majority_sampled_C1 = X_majority[sampled_indices_random]\n",
    "y_majority_sampled_C1 = y_majority[sampled_indices_random]\n",
    "\n",
    "X_majority_sampled_C2 = X_majority[sampled_indices_centroid]\n",
    "y_majority_sampled_C2 = y_majority[sampled_indices_centroid]\n",
    "\n",
    "# 合并 C1、C2 和少数类\n",
    "X_resampled = np.vstack([X_minority, X_majority_sampled_C1, X_majority_sampled_C2])\n",
    "y_resampled = np.hstack([y_minority, y_majority_sampled_C1, y_majority_sampled_C2])\n",
    "\n",
    "print(f\"\\n采样后的类别分布:\")\n",
    "print(Counter(y_resampled))\n",
    "print(f\"最终训练集样本数: {len(y_resampled)}\")\n",
    "\n",
    "# 保存处理后的数据\n",
    "output_dir = os.path.join('data', 'resampled')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 步骤4: 保存为CSV格式\n",
    "\n",
    "# 将采样后的 X (NumPy数组) 转换回带列名的 DataFrame\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X_train.columns)\n",
    "\n",
    "# 添加标签列\n",
    "X_resampled_df['Label_code'] = y_resampled\n",
    "\n",
    "# 保存为CSV\n",
    "output_path_csv = os.path.join(output_dir, 'train_CBMP.csv')\n",
    "X_resampled_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(f\"\\n数据已保存为CSV文件: '{output_path_csv}'\")\n",
    "print(f\"保存的数据形状: {X_resampled_df.shape}\")\n",
    "\n",
    "# np.save(os.path.join(output_dir, 'X_train_CBMP.npy'), X_resampled)\n",
    "# np.save(os.path.join(output_dir, 'y_train_CBMP.npy'), y_resampled)\n",
    "\n",
    "# print(f\"\\n数据已保存到: {output_dir}\")\n",
    "# print(f\"- X_train_CBMP.npy: {X_resampled.shape}\")\n",
    "# print(f\"- y_train_CBMP.npy: {y_resampled.shape}\")\n",
    "\n",
    "X_train_resampled = X_resampled\n",
    "y_train_resampled = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始训练集的类别分布:\n",
      "Counter({np.int64(3): 921732, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "\n",
      "应用 SMOTE 后的类别分布:\n",
      "Counter({np.int64(5): 921732, np.int64(3): 921732, np.int64(1): 921732, np.int64(4): 921732, np.int64(0): 921732, np.int64(2): 921732})\n",
      "原始训练集样本数: 1293618\n",
      "过采样后训练集样本数: 5530392\n",
      "新增样本数: 4236774\n",
      "\n",
      "数据已保存为CSV文件: 'data\\resampled\\train_SMOTE.csv'\n",
      "保存的数据形状: (5530392, 20)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"原始训练集的类别分布:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# 应用 SMOTE 过采样\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\n应用 SMOTE 后的类别分布:\")\n",
    "print(Counter(y_train_resampled))\n",
    "print(f\"原始训练集样本数: {len(y_train)}\")\n",
    "print(f\"过采样后训练集样本数: {len(y_train_resampled)}\")\n",
    "print(f\"新增样本数: {len(y_train_resampled) - len(y_train)}\")\n",
    "\n",
    "# 保存处理后的数据\n",
    "output_dir = os.path.join('data', 'resampled')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # 方式1: 保存为 .npy 格式\n",
    "# np.save(os.path.join(output_dir, 'X_train_SMOTE.npy'), X_train_resampled)\n",
    "# np.save(os.path.join(output_dir, 'y_train_SMOTE.npy'), y_train_resampled)\n",
    "\n",
    "# print(f\"\\n数据已保存到: {output_dir}\")\n",
    "# print(f\"- X_train_SMOTE.npy: {X_train_resampled.shape}\")\n",
    "# print(f\"- y_train_SMOTE.npy: {y_train_resampled.shape}\")\n",
    "\n",
    "# 方式2: 保存为 CSV 格式\n",
    "X_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "X_resampled_df['Label_code'] = y_train_resampled\n",
    "\n",
    "output_path_csv = os.path.join(output_dir, 'train_SMOTE.csv')\n",
    "X_resampled_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(f\"\\n数据已保存为CSV文件: '{output_path_csv}'\")\n",
    "print(f\"保存的数据形状: {X_resampled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始训练集的类别分布:\n",
      "Counter({np.int64(3): 921732, np.int64(4): 183022, np.int64(5): 100505, np.int64(2): 37454, np.int64(0): 30824, np.int64(1): 20081})\n",
      "\n",
      "类别分布: {np.int64(5): 100505, np.int64(3): 921732, np.int64(1): 20081, np.int64(4): 183022, np.int64(0): 30824, np.int64(2): 37454}\n",
      "多数类: 3, 样本数: 921732\n",
      "\n",
      "类别 5: 当前样本数=100505, 需要生成=821227\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 75.3 GiB for an array with shape (100505, 100505) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m n_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(current_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, X_cls\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     44\u001b[0m kpca \u001b[38;5;241m=\u001b[39m KernelPCA(n_components\u001b[38;5;241m=\u001b[39mn_components, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     45\u001b[0m                 gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m X_cls\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, \n\u001b[0;32m     46\u001b[0m                 fit_inverse_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 47\u001b[0m X_kpca \u001b[38;5;241m=\u001b[39m \u001b[43mkpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  KPCA变换: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_cls\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_kpca\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 步骤2: 识别不重要成分（可调比例）\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py:476\u001b[0m, in \u001b[0;36mKernelPCA.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model from data in X and transform X.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m        Transformed values.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# no need to use the kernel to transform X, use shortcut expression\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     X_transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvectors_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues_)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py:441\u001b[0m, in \u001b[0;36mKernelPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_centerer \u001b[38;5;241m=\u001b[39m KernelCenterer()\u001b[38;5;241m.\u001b[39mset_output(transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# When kernel=\"precomputed\", K is X but it's safe to perform in place operations\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# on K because a copy was made before if requested by copy_X.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform_in_place(K)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py:324\u001b[0m, in \u001b[0;36mKernelPCA._get_kernel\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0}\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pairwise_kernels(\n\u001b[0;32m    325\u001b[0m     X, Y, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel, filter_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    326\u001b[0m )\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2675\u001b[0m, in \u001b[0;36mpairwise_kernels\u001b[1;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   2672\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(metric):\n\u001b[0;32m   2673\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(_pairwise_callable, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1960\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1957\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(X, Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1608\u001b[0m, in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1606\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1608\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1609\u001b[0m K \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgamma\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;66;03m# exponentiate K in-place when using numpy\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:388\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m         )\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:424\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    421\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[0;32m    426\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n",
      "File \u001b[1;32mg:\\Anaconda3\\envs\\anomaly2sign\\lib\\site-packages\\sklearn\\utils\\extmath.py:203\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    201\u001b[0m         ret \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mtensordot(a, b, axes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, b_axis])\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    206\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m ):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 75.3 GiB for an array with shape (100505, 100505) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"原始训练集的类别分布:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "# 转换为 numpy array\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "# 找出所有类别及其样本数\n",
    "class_counts = Counter(y_train_array)\n",
    "max_samples = max(class_counts.values())\n",
    "majority_class = max(class_counts, key=class_counts.get)\n",
    "\n",
    "print(f\"\\n类别分布: {dict(class_counts)}\")\n",
    "print(f\"多数类: {majority_class}, 样本数: {max_samples}\")\n",
    "\n",
    "# 初始化结果列表\n",
    "X_resampled_list = []\n",
    "y_resampled_list = []\n",
    "\n",
    "# 对每个少数类进行LICIC处理\n",
    "for cls in class_counts.keys():\n",
    "    cls_idx = y_train_array == cls\n",
    "    X_cls = X_train_array[cls_idx]\n",
    "    y_cls = y_train_array[cls_idx]\n",
    "    \n",
    "    current_samples = len(X_cls)\n",
    "    samples_needed = max_samples - current_samples\n",
    "    \n",
    "    print(f\"\\n类别 {cls}: 当前样本数={current_samples}, 需要生成={samples_needed}\")\n",
    "    \n",
    "    # 保留原始样本\n",
    "    X_resampled_list.append(X_cls)\n",
    "    y_resampled_list.append(y_cls)\n",
    "    \n",
    "    if samples_needed > 0 and current_samples < max_samples:  # 少数类需要过采样\n",
    "        # 步骤1: 使用 KPCA 进行核主成分分析\n",
    "        n_components = min(current_samples - 1, X_cls.shape[1], 20)\n",
    "        kpca = KernelPCA(n_components=n_components, kernel='rbf', \n",
    "                        gamma=1.0 / X_cls.shape[1], random_state=42, \n",
    "                        fit_inverse_transform=True)\n",
    "        X_kpca = kpca.fit_transform(X_cls)\n",
    "        \n",
    "        print(f\"  KPCA变换: {X_cls.shape} -> {X_kpca.shape}\")\n",
    "        \n",
    "        # 步骤2: 识别不重要成分（可调比例）\n",
    "        less_important_ratio = 0.5  # ← 修改：设为可调参数\n",
    "        n_less_important = max(1, int(n_components * less_important_ratio))\n",
    "        important_indices = np.array(range(n_components - n_less_important))\n",
    "        less_important_indices = np.array(range(n_components - n_less_important, n_components))\n",
    "        \n",
    "        print(f\"  重要成分数量: {len(important_indices)}\")\n",
    "        print(f\"  不重要成分数量: {len(less_important_indices)}\")\n",
    "        \n",
    "        # 步骤3: 生成新样本\n",
    "        synthetic_samples_kpca = []\n",
    "        \n",
    "        for _ in range(samples_needed):\n",
    "            # 随机选择基础样本（用于重要成分）\n",
    "            base_idx = np.random.choice(len(X_kpca))\n",
    "            new_sample = X_kpca[base_idx].copy()\n",
    "            \n",
    "            # 重要成分：完全保持不变（已经在copy中保留）\n",
    "            \n",
    "            # 不重要成分：从多个样本中组合\n",
    "            # 随机选择另外2个样本用于不重要成分的混合\n",
    "            idx1, idx2 = np.random.choice(len(X_kpca), 2, replace=True)\n",
    "            sample1_less_imp = X_kpca[idx1][less_important_indices]\n",
    "            sample2_less_imp = X_kpca[idx2][less_important_indices]\n",
    "            \n",
    "            # 对这些不重要成分进行排列\n",
    "            permuted_indices_1 = np.random.permutation(len(less_important_indices))\n",
    "            permuted_indices_2 = np.random.permutation(len(less_important_indices))\n",
    "            \n",
    "            # 线性组合排列后的不重要成分\n",
    "            beta = np.random.uniform(0.0, 1.0)\n",
    "            new_sample[less_important_indices] = (\n",
    "                beta * sample1_less_imp[permuted_indices_1] +\n",
    "                (1 - beta) * sample2_less_imp[permuted_indices_2]\n",
    "            )\n",
    "            \n",
    "            synthetic_samples_kpca.append(new_sample)\n",
    "        \n",
    "        synthetic_samples_kpca = np.array(synthetic_samples_kpca)\n",
    "        \n",
    "        # 步骤4: 使用 KPCA 的逆变换映射回原始空间\n",
    "        try:\n",
    "            X_synthetic = kpca.inverse_transform(synthetic_samples_kpca)\n",
    "            print(f\"  成功使用KPCA逆变换生成 {samples_needed} 个合成样本\")\n",
    "        except Exception as e:\n",
    "            # 如果逆变换失败，使用近似方法\n",
    "            print(f\"  警告: KPCA逆变换失败 ({str(e)})，使用最近邻近似方法\")\n",
    "            X_synthetic = np.zeros((samples_needed, X_cls.shape[1]))\n",
    "            for i, sample_kpca in enumerate(synthetic_samples_kpca):\n",
    "                distances = np.linalg.norm(X_kpca - sample_kpca, axis=1)\n",
    "                nearest_idx = np.argmin(distances)\n",
    "                X_synthetic[i] = X_cls[nearest_idx]\n",
    "            print(f\"  使用近似方法生成 {samples_needed} 个合成样本\")\n",
    "        \n",
    "        y_synthetic = np.full(samples_needed, cls)\n",
    "        \n",
    "        X_resampled_list.append(X_synthetic)\n",
    "        y_resampled_list.append(y_synthetic)\n",
    "\n",
    "# 合并所有类别\n",
    "X_train_resampled = np.vstack(X_resampled_list)\n",
    "y_train_resampled = np.hstack(y_resampled_list)\n",
    "\n",
    "print(f\"\\n应用 LICIC 后的类别分布:\")\n",
    "print(Counter(y_train_resampled))\n",
    "print(f\"原始训练集样本数: {len(y_train_array)}\")\n",
    "print(f\"过采样后训练集样本数: {len(y_train_resampled)}\")\n",
    "print(f\"新增样本数: {len(y_train_resampled) - len(y_train_array)}\")\n",
    "\n",
    "# 保存处理后的数据\n",
    "output_dir = './resampled_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存为CSV格式\n",
    "X_resampled_df = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n",
    "X_resampled_df['Label_code'] = y_train_resampled\n",
    "\n",
    "output_path_csv = os.path.join(output_dir, 'train_LICIC.csv')\n",
    "X_resampled_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(f\"\\n数据已保存为CSV文件: '{output_path_csv}'\")\n",
    "print(f\"保存的数据形状: {X_resampled_df.shape}\")\n",
    "\n",
    "# 可选：同时保存为npy格式（取消注释以启用）\n",
    "# np.save(os.path.join(output_dir, 'X_train_LICIC.npy'), X_train_resampled)\n",
    "# np.save(os.path.join(output_dir, 'y_train_LICIC.npy'), y_train_resampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly2sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
